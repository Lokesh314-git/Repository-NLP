{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 15**  - Text Summarizer\n",
        "\n",
        "**Aim :**\n",
        "\n",
        "To generate a concise summary of a large text using NLP techniques.\n",
        "\n",
        "**Procedure**\n",
        "\n",
        "Import nltk and heapq.\n",
        "\n",
        "Load or input a text document.\n",
        "\n",
        "Tokenize text into sentences and words.\n",
        "\n",
        "Compute word frequencies, ignoring stopwords.\n",
        "\n",
        "Score each sentence based on important words.\n",
        "\n",
        "Select top sentences to form the summary.\n",
        "\n",
        "Display summarized text."
      ],
      "metadata": {
        "id": "qBSvbI4rxqzp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AqCGNRd636Tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7562fb-5d9c-4a5e-e287-8177c0878652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "\n",
            "\n",
            "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines. Machine Learning, a subset of AI, allows systems to learn from data without explicit programming. Natural Language Processing enables machines to understand and interpret human language.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import heapq\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines.\n",
        "It has applications in various fields such as healthcare, finance, education, and autonomous vehicles.\n",
        "Machine Learning, a subset of AI, allows systems to learn from data without explicit programming.\n",
        "Natural Language Processing enables machines to understand and interpret human language.\n",
        "AI continues to grow rapidly and impacts everyday life in multiple ways.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "words = word_tokenize(text.lower())\n",
        "\n",
        "\n",
        "word_frequencies = {}\n",
        "\n",
        "for word in words:\n",
        "    if word not in stop_words and word.isalpha():\n",
        "        word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "sentence_scores = {}\n",
        "\n",
        "for sent in sentences:\n",
        "    for word in word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies:\n",
        "            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_frequencies[word]\n",
        "\n",
        "\n",
        "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
        "summary = \" \".join(summary_sentences)\n",
        "\n",
        "\n",
        "print(\"Summary:\\n\")\n",
        "print(summary)\n"
      ]
    }
  ]
}